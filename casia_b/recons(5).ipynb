{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import optim,nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import copy\n",
    "plt.ion()\n",
    "\n",
    "from loess import Loess\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained resnet model for feature vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 125\n",
    "\n",
    "#pretrained model\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "#change last layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "print(num_ftrs)\n",
    "model_conv.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "#load model\n",
    "PATH = \"/home/nirbhay/tharun/casia_b/rs18_nm14_ft_fe.pth\"\n",
    "model_conv.load_state_dict(torch.load(PATH,map_location='cpu'))\n",
    "\n",
    "# Use the model object to select the desired layer\n",
    "layer = model_conv._modules.get('avgpool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model_conv.eval()\n",
    "\n",
    "scaler = transforms.Resize((224, 224))\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(image_name):\n",
    "    # 1. Load the image with Pillow library\n",
    "    img = Image.open(image_name)    \n",
    "    img = img.convert('RGB')\n",
    "    # 2. Create a PyTorch Variable with the transformed image\n",
    "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))    \n",
    "    # 3. Create a vector of zeros that will hold our feature vector\n",
    "    #    The 'avgpool' layer has an output size of 512\n",
    "    my_embedding = torch.zeros(512)    \n",
    "    # 4. Define a function that will copy the output of a layer\n",
    "    def copy_data(m, i, o):\n",
    "        my_embedding.copy_(o.data[0,:,0,0])    \n",
    "    # 5. Attach that function to our selected layer\n",
    "    h = layer.register_forward_hook(copy_data)    \n",
    "    # 6. Run the model on our transformed image\n",
    "    model_conv(t_img)   \n",
    "    # 7. Detach our copy function from the layer\n",
    "    h.remove()   \n",
    "    # 8. Return the feature vector\n",
    "    return my_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained model to map latent vectors to person id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class latent_vect(nn.Module):\n",
    "    def __init__(self,in_sz,out_sz):\n",
    "        super(latent_vect, self).__init__()\n",
    "        self.fc = nn.Linear(in_sz,out_sz)\n",
    "    def forward(self,x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "model_lat = latent_vect(512,125)\n",
    "PATH = \"/home/nirbhay/tharun/casia_b/lt_fe_ft.pth\"\n",
    "model_lat.load_state_dict(torch.load(PATH))\n",
    "model_lat.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_corr = 0\n",
    "glob_pics = 0\n",
    "\n",
    "rank = [0 for i in range(11)]\n",
    "\n",
    "def find_miss(y,app):\n",
    "        y = np.array(y)\n",
    "        x = [i for i in range(y.shape[0])]\n",
    "        x = np.array(x)\n",
    "        \n",
    "        #split data into trianing and test or missing data\n",
    "        fx,vx,fy,vy = train_test_split(x,y,test_size=0.2, random_state=4)#try shuffle on off\n",
    "# #         print(f\"train {fx.shape} {fy.shape} val {vx.shape} {vy.shape}\")\n",
    "        \n",
    "        fx, fy = zip(*sorted(zip(fx, fy)))\n",
    "        vx, vy = zip(*sorted(zip(vx, vy)))\n",
    "        \n",
    "        fx = np.array(fx)\n",
    "        vx = np.array(vx)\n",
    "        fy = np.array(fy)\n",
    "        vy = np.array(vy)\n",
    "        \n",
    "#         pred_y = np.zeros(vy.shape)\n",
    "        print(f\"complete cyc {y.shape} non-occ {fy.shape}\")\n",
    "        gait = np.zeros(512)\n",
    "        \n",
    "        pred_y = np.zeros(vy.shape)\n",
    "        \n",
    "        for i,ind in enumerate(fx):\n",
    "            gait+=fy[i]\n",
    "        \n",
    "        for i in range(fy.shape[1]):\n",
    "            loess = Loess(fx, fy[:,i])\n",
    "            for j,gx in enumerate(vx):\n",
    "                pred_y[j,i] = loess.estimate(gx, window=5)\n",
    "        \n",
    "#         print(\"pred_y shape\",pred_y.shape)\n",
    "        \n",
    "        for i,ind in enumerate(vx):\n",
    "            gait+=pred_y[i]\n",
    "\n",
    "        gei = gait/y.shape[0]\n",
    "    \n",
    "        #debugging\n",
    "#         gei = y.mean(0)\n",
    "#         print(\"latent gei vector \",gei.shape)\n",
    "        \n",
    "        \n",
    "        tensor_gei = torch.from_numpy(gei).float()\n",
    "        tensor_gei = torch.unsqueeze(tensor_gei, 0)\n",
    "#         print(\"latent gei tensor \",tensor_gei.size())\n",
    "        with torch.set_grad_enabled(False):\n",
    "            ot = model_lat(tensor_gei)\n",
    "            _, preds = torch.max(ot, 1)\n",
    "        \n",
    "        probs = ot.numpy()\n",
    "        ids = np.argsort(-probs,axis=1)\n",
    "        \n",
    "        for i in range(10):\n",
    "            if ids[0][i] == int(app):\n",
    "                rank[i+1] +=1\n",
    "                break;\n",
    "        \n",
    "        \n",
    "        print(f\"prediction {preds} label {app}\")\n",
    "        print(preds==int(app))\n",
    "        global glob_corr\n",
    "        global glob_pics\n",
    "        if preds==int(app):\n",
    "            glob_corr +=1\n",
    "        glob_pics += 1\n",
    "        print(\"^\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find gait nm-05 nm-06\n",
    "with open(\"indices_gait.txt\", \"rb\") as fl:\n",
    "    ind = pickle.load(fl)\n",
    "    \n",
    "#gait energy --helper function\n",
    "def find_gait(path,app,nm):\n",
    "    files = glob.glob(path+\"*.png\")\n",
    "    files.sort()\n",
    "    \n",
    "    num_gait=0\n",
    "    for j in range(len(ind[int(app)][nm-1])-2):\n",
    "        if j is None:\n",
    "            continue\n",
    "        c=0\n",
    "        #all images in gait cycle\n",
    "        y = []\n",
    "        for i in range(ind[int(app)][nm-1][j],ind[int(app)][nm-1][j+2]+1):\n",
    "            img = cv2.imread(files[i],0)\n",
    "            y.append(get_vector(files[i]).numpy())\n",
    "        #predict missing latent vector\n",
    "        find_miss(y,app)\n",
    "        num_gait+=1\n",
    "    print(f\"num_gaits {num_gait}\")\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#gait energy images\n",
    "for i in tqdm(range(1,125)):\n",
    "    if i<10:\n",
    "        app = \"00\"+str(i)\n",
    "    elif i<100:\n",
    "        app = \"0\"+str(i)\n",
    "    else :\n",
    "        app = str(i)\n",
    "    for j in range(5,7):\n",
    "        path = \"/SSD/Pratik/Gait_Data/Casia_data_preprocessed/GaitDatasetB-silh_PerfectlyAlingedFullPossibleCyclesImages/\"+app+\"/nm-0\"+str(j)+\"/\"\n",
    "        print(f\"person {app} nm {j}\")\n",
    "        find_gait(path,app,j)\n",
    "    print(\"*\"*20)\n",
    "\n",
    "print(f\"accuracy {glob_corr/glob_pics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    rank[i]+=rank[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    rank[i]/=glob_pics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}